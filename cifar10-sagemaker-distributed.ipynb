{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Amazon SageMaker\n",
    "\n",
    "In this notebook we use the SageMaker Python SDK to setup and run a distributed training job.\n",
    "SageMaker makes it easy to train models across a cluster containing a large number of machines, without having to explicitly manage those resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import essentials packages, start a sagemaker session and specify the bucket name you created in the pre-requsites section of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name    = 'sagemaker-jobs'\n",
    "jobs_folder    = 'jobs'\n",
    "dataset_folder = 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Specify hyperparameters, instance type and number of instances to distribute training to. The `hvd_processes_per_host` corrosponds to number of GPUs per instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed training with a total of 32 workers: 4 instances of ml.p3.16xlarge\n",
      "8 GPU(s) per instance\n"
     ]
    }
   ],
   "source": [
    "hvd_instance_type = 'ml.p3.16xlarge'\n",
    "hvd_instance_count = 4\n",
    "hvd_processes_per_host = 8\n",
    "\n",
    "print(f'Distributed training with a total of {hvd_processes_per_host*hvd_instance_count} workers: {hvd_instance_count} instances of {hvd_instance_type}')\n",
    "print(f'{hvd_processes_per_host} GPU(s) per instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name   = f'tf-horovod-{hvd_instance_count}x{hvd_processes_per_host}-workers-{time.strftime(\"%Y-%m-%d-%H-%M-%S-%j\", time.gmtime())}'\n",
    "output_path = f's3://{bucket_name}/{jobs_folder}'\n",
    "tboard_logs = f's3://{bucket_name}/tensorboard_logs/{job_name}'\n",
    "\n",
    "metric_definitions = [{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}]\n",
    "\n",
    "hyperparameters = {'epochs': 100, \n",
    "                   'learning-rate': 0.001,\n",
    "                   'momentum': 0.9,\n",
    "                   'weight-decay': 2e-4,\n",
    "                   'optimizer': 'adam',\n",
    "                   'batch-size' : 256}\n",
    "\n",
    "sm_config       = {'tensorboard_logs': tboard_logs}\n",
    "\n",
    "hyperparameters.update(sm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** In this cell we create a SageMaker estimator, by providing it with all the information it needs to launch instances and execute training on those instances.\n",
    "\n",
    "Since we're using horovod for distributed training, we specify `distributions` to mpi which is used by horovod.\n",
    "\n",
    "In the TensorFlow estimator call, we specify training script under `entry_point` and dependencies under `code`. SageMaker automatically copies these files into a TensorFlow container behind the scenes, and are executed on the training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {\n",
    "                 'mpi': {\n",
    "                          'enabled'           : True,\n",
    "                          'processes_per_host': hvd_processes_per_host,\n",
    "                          'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO -x OMPI_MCA_btl_vader_single_copy_mechanism=none'\n",
    "                        }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "hvd_estimator = TensorFlow(entry_point         = 'cifar10-tf-horovod-sagemaker.py', \n",
    "                          source_dir           = 'code',\n",
    "                          output_path          = output_path + '/',\n",
    "                          code_location        = output_path,\n",
    "                          role                 = role,\n",
    "                          train_instance_count = hvd_instance_count, \n",
    "                          train_instance_type  = hvd_instance_type,\n",
    "                          framework_version    = '1.15', \n",
    "                          py_version           = 'py3',\n",
    "                          script_mode          = True,\n",
    "                          metric_definitions   = metric_definitions,\n",
    "                          hyperparameters      = hyperparameters,\n",
    "                          distributions        = distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Specify dataset locations in Amazon S3 and then call the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f's3://{bucket_name}/{dataset_folder}/cifar10-dataset/train'\n",
    "val_path   = f's3://{bucket_name}/{dataset_folder}/cifar10-dataset/validation'\n",
    "eval_path  = f's3://{bucket_name}/{dataset_folder}/cifar10-dataset/eval'\n",
    "\n",
    "hvd_estimator.fit({'train': train_path,'validation': val_path,'eval': eval_path}, \n",
    "                  job_name=job_name, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in the `estimator_hvd.fit()` function above, change`wait=True` if you want to see the training output in the Jupyter notebook.\n",
    "Advantage of setting `wait=False`, is that you can continue to run cells. \n",
    "Since we're unblocked due to `wait=False` we can now launch tensorboard in the notebook and monitor progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Monitor progress on TensorBoard. Launch tensorboard server and open the link on a new tab to visualize training progress, and navigate to the following link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !S3_REGION=us-west-2 tensorboard --logdir s3://{bucket_name}/tensorboard_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new browser and navigate to the folloiwng link to access TensorBoard:\n",
    "<br> https://localhost:6006\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
